---
title: 告警平台方案调研
date: 2018-10-13 14:23:48
categories: coding
tags:
  - 告警平台
---

本文是关于告警平台的能力以及方案，设计思路的调研

<!--more-->

## 数据源

### 阿里监控系统 Sunfire

操作系统，JVM，中间件等，大多数业务的监控指标都从应用的日志中抽取。

### 腾讯天网

1. 主动：所有的业务开发出来的应用程序，在上线之前能够按照运维的要求，或者自身对代码质量的监控，提前做好了很多埋点。
2. 被动：一种从外部探测而非自主上报的被动行为。
3. 旁路：舆情监控等跟第三方的对比数据，来间接的反应我们外网服务的真实情况

#### 衡量监控点

**请求量、成功率、延时**

#### 全链路监控

![](http://tektea-img.b0.upaiyun.com/blog/2016/07/SNG%E7%9B%91%E6%8E%A7%E5%85%A8%E6%99%AF%E5%A6%82.png)

#### 数据上报

![](http://tektea-img.b0.upaiyun.com/blog/2016/07/143.png)

运营开发更专注于对Storm逻辑的一些封装，专注于原始数据的高效加工处理，然后，告诉数据消费者（运维）有什么样的数据，在数据银行中提供了哪些数据的类型，提供了哪些丰富的接口，所有产品化的工作都是由运维来实现的。




## 架构

![](http://tektea-img.b0.upaiyun.com/blog/2017/12/03.jpg)

传统的日志监控，现在大多数监控平台采用的一个方案。Agnet 检测日志变化增量推送，经过消息中间件如 kafka，流式计算引擎如 Jstorm/flink 去消费 kafka 产生出来的数据，中间的流式计算可能有多步的处理，最后流向 DB，很传统的架构。

这个架构最麻烦的是我不知道什么时候数据已经全部到齐了。如果机器很多，agent 返回数据的时间并不确定, 要保证所有机器日志采齐了数据才准确，这在流式计算里很难处理。

解决方案参考

```
https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101
https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102
```

### 阿里监控系统 Sunfire

#### 功能结构

[阿里万亿交易量级下的秒级监控](http://www.yunweipai.com/archives/24081.html)

![](http://tektea-img.b0.upaiyun.com/blog/2017/12/04.jpg)

#### 架构

![](http://tektea-img.b0.upaiyun.com/blog/2017/12/05.jpg)

有三个角色 Brain、Reduce 和 Map。这三个角色我们统称为计算模块。

onfigDB 里面配置了监控项。监控项会定义配置需要从哪个应用、哪个路径采集日志、采集回来的日志应该做哪些的处理、根据什么样的规则进行计算。Brain 会按照周期从 ConflgDB 里读取配置，生成拓扑。然后安装到 Reduce 上面，Reduce 把拓扑再分解成它的子任务，再安装到 Map 上面，最后 map 去拉日志。

### 阿里妈妈 Goldeneye

![](https://res.infoq.com/articles/alibaba-goldeneye-four-links/zh/resources/01.jpg)

Goldeneye监控系统的四个输入：**实时监测数据、历史数据、预测策略、报警过滤规则**。

**预测策略主要包括**：

1. 阈值参数：设置基于预测基准值的系数决定阈值上下限区间、分时段阈值预测系数、分报警灵敏度阈值预测系数；
2. 预测参数：样本数量、异常样本过滤的高斯函数水位或者过滤比例、基于均值漂移模型的样本分段选取置信度等。

**报警过滤规则**：

主要是为了在充分捕捉疑似异常点的前提下，过滤不必要的报警。比如指标M1异常，但是组合规则是M1和M2同时异常才报警，这种就会过滤掉。

再比如，按照报警收敛规则，一个监控项的第1次，第2次，第10次，第50次连续报警值得关注

![](https://res.infoq.com/articles/alibaba-goldeneye-four-links/zh/resources/02.jpg)

#### 架构演进

[阿里Goldeneye业务监控平台之架构演进，如何实时处理100T+/天的日志量？](http://www.infoq.com/cn/articles/how-to-deal-with-100t-one-day-log)

总结是：

从每个业务线单独开发逻辑，部署任务，自己写拿 Hbase 数据，解析日志，等逻辑；到封装 `write`、`join`、`topN` 逻辑给开发人员调用；到开发人员写 SQL 语句，通过解析 SQL 来实现逻辑

## 并发

### 阿里妈妈 Goldeneye

参考 [阿里Goldeneye业务监控平台之架构演进，如何实时处理100T+/天的日志量？
](http://www.infoq.com/cn/articles/how-to-deal-with-100t-one-day-log) 这篇文章


## 监控

### 阿里妈妈 Goldeneye

对于业务监控人工维护阈值就比较复杂，需要有丰富的经验来拍定阈值，需要人工持续的维护不同监控项的监控阈值。所以，在业务快速发展的前提下，传统的静态阈值监控很容易出现了误报、漏报的问题，而且人工维护成本高


**智能监控就是让系统在业务监控的某些环节上代替人工执行和判断的过程**。人工维护监控目标和阈值是以经验为参考的，系统如何自动判断哪些目标需要监控、自动设定监控目标的阈值水位、不用人力维护，是基于对历史样本数据统计分析得出判断依据。

#### 发现异常点

让程序自动对监控项指标的基准值、阈值做预测，在检测判断异常报警时使用规则组合和均值漂移算法，能精确地判断需要报警的异常点和变点。

**以往做法：**

1. 给指标M1设置一个水位线，低于（或高于）水位，触发报警；
2. 给指标M1设置同比、环比波动幅度，比如同比波动20%、环比波动10%触发报警；

Goldeneye 做法：

**动态阈值**

动态阈值是为控制图的时间序列每个点，预估该点对应时刻这个指标的基准值、阈值上限、阈值下限，从而让程序可以自动判断是否有异常

![](https://res.infoq.com/articles/alibaba-goldeneye-four-links/zh/resources/03.jpg)

**变点检测**

就是持续微量下跌到一定时间，累积变化量到一定程度后，使得变点前后监测指标在一段时间内的均值发生漂移。

变点检测弥补了动态阈值对细微波动的检测不足，这两种方式结合起来，基本可以达到不漏报和不误报的平衡

**辅助定位**

全链路tracing：全链路分析有两种数据记录方式，要么链路每个节点内部透传，拼接成完整链路处理信息记录到最终的节点日志；要么异步地每个节点各自将信息push到中间件

报警时间点上发生了什么：把运维事件、运营调整事件尽可能地收集起来，将这些事件地散点图和监测报警的控制图结合起来

**时间序列平稳化**

1. 滑动平均：比如波动锯齿明显，容易造成误报干扰的化，则加大监控监测周期，将5分钟提高到30分钟
2. 持续报警判断：如果觉得30分钟发现问题会比较晚，可以按5分钟检测，锯齿波动容易发生报警，但可以连续3次报警再发通知

#### 监控项自动发现

判断如何筛选监控项的规则交给系统，让它去定期检查哪些监控项已经实效，哪些监控项需要新增，哪些监控项的阈值需要调节。

#### 过滤误报

**对误报case欲擒故纵，在首先确保不漏报的基础上降低误报率。**

这一环节我们基于动态阈值去检测时相对严格一些（或者说这一环节不用考虑报警收敛的问题），然后对这些疑似异常点再做验证、过滤，最终生成报警通知，验证和过滤的依据是预先定义的规则，比如指标组合判断、报警收敛表达式等。

### 腾讯天网

[腾讯亿万量级告警是如何做到全、准、快的？](http://www.yunweipai.com/archives/8359.html)

#### ROOT智能监控系统

我们的DB挂了，会层层往前推，我们的逻辑层、接入层、负载均衡，甚至到我们的用户端报上的成功率都会受到影响。

但是运维并不希望收到这N个现象告警，我们希望把DB宕机的根源告警发出来，其他告警都收敛掉。

![](http://tektea-img.b0.upaiyun.com/blog/2016/07/223.png)

基于我们的业务拓扑图，根据时间的相关性，把告警都叠加在链路上，把一些不需要关注的点都过滤掉，最后得到一个经过经验分析的模型。

里面包括了：**降维策略、时间相关性分析、权重面积分析**，具体可以参考文章内容

## 参考

* [美团外卖自动化业务运维系统建设](https://tech.meituan.com/digger_share.html)
* [美团外卖客户端高可用建设体系](https://tech.meituan.com/waimai_client_high_availability.html)
* [美团酒旅实时数据规则引擎应用实践](https://tech.meituan.com/hb_rt_operation.html)
* [美团云运维：如何承载千万级云计算基础服务](https://www.jianshu.com/p/eaf467963cdb)
* [在架构师眼里，一份美团外卖是如何做出来的？](https://zhuanlan.zhihu.com/p/35995592)
* [阿里万亿交易量级下的秒级监控](http://www.yunweipai.com/archives/24081.html)
* [从人肉到智能，阿里运维体系经历了哪些变迁？](https://102.alibaba.com/detail/?id=184)
* [阿里Goldeneye四个环节落地智能监控：预测、检测、报警及定位](http://www.infoq.com/cn/articles/alibaba-goldeneye-four-links)
* [阿里Goldeneye业务监控平台之架构演进，如何实时处理100T+/天的日志量？](http://www.infoq.com/cn/articles/how-to-deal-with-100t-one-day-log)
* [腾讯亿万量级告警是如何做到全、准、快的？](http://www.yunweipai.com/archives/8359.html)
* [腾讯SNG全链路日志监控平台之构建挑战](https://zhuanlan.zhihu.com/p/31243227)
* [每天5万条告警，腾讯如何做到“咖啡运维”？](https://juejin.im/entry/5b98e74de51d450e9c554394)
* [DockOne技术分享（十四）：腾讯蓝鲸数据平台之告警系统](http://dockone.io/article/537)
* [统一监控报警平台的架构设计思路分享](http://os.51cto.com/art/201603/507858.htm)
* [系统化全方位监控告警，这一篇足矣](http://www.10tiao.com/html/249/201803/2651961017/1.html)
* [如何打造大规模互联网企业的监控告警平台——以携程 hickwall 为例](http://www.infoq.com/cn/presentations/how-to-build-a-large-scale-internet-enterprise-monitoring-alarm-platform)
* [虚拟座谈会：聊聊AIOps的终极价值](http://www.infoq.com/cn/news/2017/09/talk-about-aiops)


